---
title: "EDA Cart Abandonment"
author: "Ethan Aslami, Riley Walburger, Tara Connin"
date: 10/5/2025
format: 
  html:
    toc: true
    toc-depth: 3
    number-sections: true
    number-depth: 2
    toc-location: left
    toc-title: "Contents"
    code-fold: true
    code-summary: "üñ±Ô∏èüëÜ Click to Show Code"
    embed-resources: true
execute:
  include: true
  eval: true    
  warning: false
  message: false---
---

# üéØ Business Problem Statement
## Business Problem

Swire Coca-Cola operates the MyCoke360 platform to streamline business-to-business ordering and improve digital engagement. Despite strong platform adoption, many customers add items to their carts but fail to complete purchases before the order cutoff time. These incomplete transactions, known as cart abandonment, reduce potential revenue and signal inefficiencies in the online ordering process.

The company seeks to better understand the factors associated with incomplete purchases in order to improve conversion rates and enhance the performance of its e-commerce platform. By examining customer activity and purchase behavior within MyCoke360, patterns can be identified that distinguish completed orders from those that are abandoned, providing a foundation for data-driven strategies to improve order completion.

## Analytic Problem

The analytic challenge is to transform raw behavioral and transactional data into a reliable view of customer purchase behavior. Google Analytics captures event-level activity such as ‚Äúadd_to_cart,‚Äù ‚Äúremove_from_cart,‚Äù and ‚Äúpurchase,‚Äù but does not record a running cart total. Therefore, the analysis must reconstruct each customer‚Äôs cart over time to determine whether the customer purchased the items before their cutoff date or left them unpurchased. A binary target variable will be created to indicate whether a customer failed to complete a purchase (1) or successfully converted (0) within an order window. Exploratory analysis will then identify patterns and predictors of abandonment, including device type, number of cart interactions, timing of last activity, item categories, and delivery frequency. This will provide the foundation for predictive and behavioral models that quantify the likelihood of cart abandonment and inform interventions to reduce it.


# üõ†Ô∏è Setup 

- Read in Files 
- Load Libraries
- Clean some of the data

```{r}
# Load Libraries 
library(tidyverse)
library(janitor)
library(caret)
library(data.table)
library(knitr)
library(kableExtra)
library(scales)

# We need to create a new dataset that uses the google analytics and the order table to create each row as a unique
# visit to the website, we will be able to tell maybe using date and time to see if they purchased or abandoned
```

## Load data 
```{r}
# some minimal cleaning here as well 

customer <- fread("C:/Users/Ethan/OneDrive - University of Utah/MSBA Program/Final Capstone Swire/data/customer.csv") |> 
  clean_names() |> 
  rename(customer_id = customer_number, # match google analytics
         plant_id = sales_office) # match the orders table and cutoff table

# Fix the cutoff time to the same date format as the GA table
cutoff_times <- fread("C:/Users/Ethan/OneDrive - University of Utah/MSBA Program/Final Capstone Swire/data/cutoff_times.csv") |> 
  clean_names() 

material <- fread("C:/Users/Ethan/OneDrive - University of Utah/MSBA Program/Final Capstone Swire/data/material.csv") |> 
  clean_names()

# Fix a few names
operating_hours <- fread("C:/Users/Ethan/OneDrive - University of Utah/MSBA Program/Final Capstone Swire/data/operating_hours.csv") |> 
  clean_names() |> 
  rename(customer_id = customer_number, 
         frequency_weeks =  frequency) # match names for a join later

orders <- fread("C:/Users/Ethan/OneDrive - University of Utah/MSBA Program/Final Capstone Swire/data/orders.csv") |> 
  clean_names()

sales <- fread("C:/Users/Ethan/OneDrive - University of Utah/MSBA Program/Final Capstone Swire/data/sales.csv") |> 
  clean_names()

# Clean up the frequency column
visit_plan <- fread("C:/Users/Ethan/OneDrive - University of Utah/MSBA Program/Final Capstone Swire/data/visit_plan.csv") |> 
  clean_names() |> 
  rename(plant_id = sales_office,
         distribution_mode_key = distribution_mode)

google_analytics <- fread("C:/Users/Ethan/OneDrive - University of Utah/MSBA Program/Final Capstone Swire/data/google_analytics.csv") |> 
  clean_names() 
```


## Clean visit plan and cutoff_times 
```{r}
# Create frequency_weeks column to match the operating hours table
visit_plan[frequency %in% c("01", "Every Week On", "1"), frequency_weeks := "Every Week"]
visit_plan[frequency %in% c("02", "Every Second Week On", "2"), frequency_weeks := "Every 2 Weeks"]
visit_plan[frequency %in% c("03", "Every Third Week On", "3"), frequency_weeks := "Every 3 Weeks"]
visit_plan[frequency %in% c("04", "Every Fourth Week On", "4"), frequency_weeks := "Every 4 Weeks"]

# Create distribution mode description column to match the other tables formatting
visit_plan[distribution_mode_key %in% c("SL"), distribution_mode_description := "Sideload"]
visit_plan[distribution_mode_key %in% c("OF"), distribution_mode_description := "OFS"]
visit_plan[distribution_mode_key %in% c("EZ"), distribution_mode_description := "EZ Pallet"]
visit_plan[distribution_mode_key %in% c("RD"), distribution_mode_description := "Rapid Delivery"]
visit_plan[distribution_mode_key %in% c("NR"), distribution_mode_description := "Night Rapid Delivery"]
visit_plan[distribution_mode_key %in% c("FS"), distribution_mode_description := "Full Service"]
visit_plan[distribution_mode_key %in% c("BK"), distribution_mode_description := "Bulk Distribution"]
visit_plan[distribution_mode_key %in% c("SE"), distribution_mode_description := "Special Events"]
visit_plan[distribution_mode_key %in% c("NS"), distribution_mode_description := "Night Side Load"]
visit_plan[distribution_mode_key %in% c("NO"), distribution_mode_description := "Night OFS"]
visit_plan[distribution_mode_key %in% c("DD"), distribution_mode_description := ""]
visit_plan[distribution_mode_key %in% c("null", ""), distribution_mode_description := ""]

# a few of these for cutoff_times as well 
cutoff_times[distribution_mode %in% c("Night Sideload"), distribution_mode_description := "Night Side Load"]
cutoff_times[distribution_mode %in% c("OFS"), distribution_mode_description := "OFS"]
cutoff_times[distribution_mode %in% c("E Pallet"), distribution_mode_description := "EZ Pallet"]
cutoff_times[distribution_mode %in% c("Nights Bulk"), distribution_mode_description := "Nights Bulk"]
cutoff_times[distribution_mode %in% c("Tell Sell"), distribution_mode_description := "Tell Sell"]
cutoff_times[distribution_mode %in% c("Sideload"), distribution_mode_description := "Sideload"]
cutoff_times[distribution_mode %in% c("Night Rapid Delivery"), distribution_mode_description := "Night Rapid Delivery"]
cutoff_times[distribution_mode %in% c("Full Service"), distribution_mode_description := "Full Service"]
cutoff_times[distribution_mode %in% c("Bulk Distribution"), distribution_mode_description := "Bulk Distribution"]
cutoff_times[distribution_mode %in% c("Special Events"), distribution_mode_description := "Special Events"]
cutoff_times[distribution_mode %in% c("Night OFS"), distribution_mode_description := "Night OFS"]
cutoff_times[distribution_mode %in% c("Rapid Delivery"), distribution_mode_description := "Rapid Delivery"]

# Update the values for shipping condition desc
visit_plan[shipping_conditions_desc %in% c("24 Hours", "Dropsite 24 Hours"), shipping_condition_time := "24hrs"]
visit_plan[shipping_conditions_desc %in% c("48 Hours", "Dropsite 48 Hours"), shipping_condition_time := "48hrs"]
visit_plan[shipping_conditions_desc %in% c("72 Hours", "Dropsite 72 Hours"), shipping_condition_time := "72hrs"]

```


## Join cutoff_times and visit_plan

```{r}

# set keys for both tables
setkey(cutoff_times, plant_id, shipping_condition_time, distribution_mode_description)
setkey(visit_plan,   plant_id, shipping_condition_time, distribution_mode_description)

# perform the join (left join by default)
visit_plan_joined <- cutoff_times[visit_plan]

# set any missing cutoff_times to 5pm (This is what kurt said to do)
visit_plan_joined[is.na(cutofftime_c), cutofftime_c := "5:00:00 PM"]
# Parse into POSIXct (time only, we don't care about the date part on this one)
visit_plan_joined[, cutofftime_posix := parse_date_time(cutofftime_c, orders = "I:M:S p")]

# There are some empty rows after the join that we set to 5pm here, this happens because of the missing Tell Sell distribution mode 
# This exists in the cutoff times but not in the visit plans.
visit_plan_joined[, anchor_date := ymd(anchor_date)]


```

## Clean visit plan to only the rows we need 
- Remoce a bunch of data outside of the time-frame of the GA data
```{r}

# Filter to relevant date range
visit_plan <- visit_plan[elt_ts >= as.POSIXct("2024-05-31")]

result <- visit_plan[!is.na(anchor_date), 
                     .(elt_ts_min = min(elt_ts),
                       elt_ts_max = max(elt_ts)),
                     by = .(customer_id, anchor_date)]

```

## Google analytics Cleaning 
- Add the time zone fix to make sure everything is in local time for GA data 

```{r}

tz_lookup <- data.table(
  state = c("UT", "WA", "CO", "AZ", "OR", "ID", "NV", "NM", "NE", "WY"),
  tz = c(
    "US/Mountain",     # Utah
    "US/Pacific",      # Washington
    "US/Mountain",     # Colorado
    "US/Arizona",      # Arizona (no DST)
    "US/Pacific",      # Oregon
    "US/Mountain",     # Idaho
    "US/Pacific",      # Nevada
    "US/Mountain",     # New Mexico
    "US/Central",      # Nebraska
    "US/Mountain"      # Wyoming
  )
)

# add state column and join tz
customer[, state := sub(".*,\\s*", "", sales_office_description)]
customer <- tz_lookup[customer, on = "state"]

# join tz to GA data
google_analytics <- customer[google_analytics, on = .(customer_id), nomatch=0]

# Manual offsets for all except Arizona
tz_offsets <- data.table(
  tz = c("US/Central", "US/Mountain", "US/Pacific"),
  hour_offset = c(1, 2, 3)
)

# Join offsets
google_analytics <- tz_offsets[google_analytics, on = "tz"]

# Add POSIXct EST timestamp
google_analytics[, event_ts_est := ymd_hms(event_timestamp, tz = "America/New_York")]

# Handle Arizona separately
google_analytics[tz == "US/Arizona", 
  hour_offset := ifelse(dst(event_ts_est), 3, 2)] # checks if daylight savings is active and works accordingly

# Apply manual hour offset
google_analytics[, event_ts_local := event_ts_est - dhours(hour_offset)]

# Show one sample per timezone
google_analytics[, .(
  sample_est = head(event_ts_est, 2),
  sample_local = head(event_ts_local, 2)
), by = tz]


# Also check unique time zones
unique(google_analytics$tz)

```

# üåê Google Analytics EDA

::: {.callout-note}
- The following sections were completed before we were able to figure out the order window.
- It provides insight into the main tables we will use throughout the project along with visualizations and summary tables.
- These do not deal with the target variable, we plan to explore that more once we are confident we have correctly defined it.
:::

## Event Type Analysis
- What were the top event types when customers interacted with the website? 

```{r}

# Most common events
event_summary <- google_analytics |>
  count(event_name, sort = TRUE) |>
  mutate(pct = n / sum(n) * 100)

# Top 15 events
ggplot(event_summary |> head(15), 
       aes(x = reorder(event_name, n), y = n)) +
  geom_col(fill = "coral") +
  geom_text(aes(label = comma(n)), hjust = -0.1, size = 3) +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
  labs(title = "Top 15 Event Types",
       subtitle = "What actions are users taking most frequently?",
       x = NULL, y = "Number of Events") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

```

## Summary Stats for event types

```{r}

# Create the summary stats table 
summary_stats <- tibble(
  Metric = c(
    "Total Events",
    "Unique Customers",
    "Date Range",
    "Avg Events per Customer",
    "Total Purchases",
    "Desktop %",
    "Mobile %",
    "Most Common Event"
  ),
  Value = c(
    comma(nrow(google_analytics)),
    comma(n_distinct(google_analytics$customer_id)),
    paste(min(google_analytics$event_date), "to", max(google_analytics$event_date)),
    round(nrow(google_analytics) / n_distinct(google_analytics$customer_id), 1),
    comma(sum(google_analytics$event_name == "purchase")),
    paste0(round(sum(google_analytics$device_category == "desktop") / nrow(google_analytics) * 100, 1), "%"),
    paste0(round(sum(google_analytics$device_category == "mobile") / nrow(google_analytics) * 100, 1), "%"),
    event_summary$event_name[1]
  )
)

# Create the table with the kable package
summary_stats |>
  kable(format = "html", col.names = c("Metric", "Value"), escape = FALSE) |>
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
                full_width = FALSE,
                position = "left") |>
  column_spec(1, bold = TRUE, width = "30%") |>
  column_spec(2, width = "70%")

```


## Event_Time Analysis 
- When were users most active on the website?
- What time and day of the week was the most popular? 

```{r}

# Weekly event volume
weekly_events <- google_analytics |>
  mutate(week = floor_date(event_ts_local, unit = "week", week_start = 1)) |> 
  count(week) |> 
  arrange(week)

# Plot the weekly volume
ggplot(weekly_events, aes(x = week, y = n)) +
  geom_line(color = "darkred", size = 0.8) +
  geom_smooth(method = "loess", se = TRUE, alpha = 0.2) +
  scale_y_continuous(labels = comma) +
  labs(title = "Weekly Event Volume Over Time",
       subtitle = "Aggregated by week (Monday‚ÄìSunday)",
       x = "Week", y = "Number of Events") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

# Hour of day analysis
hourly_pattern <- google_analytics |>
  mutate(hour = hour(event_ts_local),
         day_of_week = wday(event_ts_local)) |>
  count(hour)

# plot the hour of day analysis in a histogram
ggplot(hourly_pattern, aes(x = hour, y = n)) +
  geom_col(fill = "coral") +
  scale_x_continuous(breaks = 0:23) +
  scale_y_continuous(labels = comma) +
  labs(title = "Event Distribution by Hour of Day",
       subtitle = "When are users most active? (Local Time)",
       x = "Hour of Day", y = "Number of Events") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

# Day of week pattern
dow_pattern <- google_analytics |>
  mutate(day_of_week = factor(
    c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")[wday(event_ts_local)],
    levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
  )) |>
  count(day_of_week)

# Visualize this to see what day of the week trends look like 
ggplot(dow_pattern, aes(x = day_of_week, y = n)) +
  geom_col(fill = "darkgreen") +
  scale_y_continuous(labels = comma) +
  labs(title = "Event Distribution by Day of Week",
       subtitle = "Are there weekday vs weekend patterns?",
       x = "Day of Week", y = "Number of Events") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

```


## Purchase Event Analysis
- These graphs are similar to the last section but now we ask the question, how did these trends play out for purchase events? 
- Purchase events are a cornerstone of this project and we thought it would be interesting to observe them for these same graph.
```{r}

# Weekly purchase event volume
weekly_purchases <- google_analytics |>
  filter(event_name == "purchase") |> 
  mutate(week = floor_date(event_ts_local, unit = "week", week_start = 1)) |> 
  count(week) |> 
  arrange(week)

ggplot(weekly_purchases, aes(x = week, y = n)) +
  geom_line(color = "darkblue", size = 0.8) +
  geom_smooth(method = "loess", se = TRUE, alpha = 0.2) +
  scale_y_continuous(labels = comma) +
  labs(title = "Weekly Purchase Volume Over Time",
       subtitle = "Purchases aggregated by week (Mon‚ÄìSun)",
       x = "Week", y = "Number of Purchases") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

# Hour of day analysis (purchase version)
hourly_pattern <- google_analytics |>
  filter(event_name == "purchase") |> 
  mutate(hour = hour(event_ts_local),
         day_of_week = wday(event_ts_local)) |>
  count(hour)

ggplot(hourly_pattern, aes(x = hour, y = n)) +
  geom_col(fill = "coral") +
  scale_x_continuous(breaks = 0:23) +
  scale_y_continuous(labels = comma) +
  labs(title = "Purchase Distribution by Hour of Day",
       subtitle = "When are users most active? (Local Time)",
       x = "Hour of Day", y = "Number of Purchases") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

# Day of week pattern (purchase version)
dow_pattern <- google_analytics |>
  filter(event_name == "purchase") |> 
  mutate(day_of_week = factor(
    c("Sunday","Monday","Tuesday","Wednesday","Thursday","Friday","Saturday")[wday(event_ts_local)],
    levels = c("Monday","Tuesday","Wednesday","Thursday","Friday","Saturday","Sunday")
  )) |>
  count(day_of_week)

ggplot(dow_pattern, aes(x = day_of_week, y = n)) +
  geom_col(fill = "darkgreen") +
  scale_y_continuous(labels = comma) +
  labs(title = "Purchase Distribution by Day of Week",
       subtitle = "Are there weekday vs weekend patterns?",
       x = "Day of Week", y = "Number of Purchases") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))


# Device category for purchases
purchase_by_device <- google_analytics |>
  filter(event_name == "purchase") |>
  count(device_category) |>
  mutate(pct = n / sum(n) * 100)

ggplot(purchase_by_device, aes(x = reorder(device_category, -n), y = n, fill = device_category)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = paste0(comma(n), "\n(", round(pct, 1), "%)")), 
            vjust = -0.3, size = 4) +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.4))) +
  scale_fill_brewer(palette = "Dark2") +
  labs(title = "Purchases by Device Category",
       subtitle = "Which devices drive more purchases?",
       x = NULL, y = "Number of Purchases") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))


```

- Purchase volume has increased steadily throughout the year of data with purchases peaking in May 2025
- Events occur mainly in the period of 12pm to 8pm local time for each customer. 
- Most Events (purchase and all others) take place on monday and event volume decreases throughout the week.
- Most purchase events occur on the desktop.

## Incomplete Purchase Analysis

::: {.callout-note}
This section is a bit more experimental, it was completed before we figured out the true order window problem but we thought it could still be interesting to leave it here.
:::

```{r}
# Define a "session" as activity within X hours 
SESSION_TIMEOUT_HOURS <- 10 
 # I set this to 10 hours for a session to give ample time to complete a transaction for customers
 # This is where things won't exatly line up with our defined order_windows later on because this solution has the potential to missrepresent order windows based on cutoff_times in the visit_plan table.


# Create sessions based on time gaps
create_sessions <- function(data) {
  data |>
    arrange(customer_id, event_timestamp) |>
    group_by(customer_id) |>
    mutate(
      time_since_last = as.numeric(difftime(event_timestamp, lag(event_timestamp), units = "hours")),
      new_session = is.na(time_since_last) | time_since_last > SESSION_TIMEOUT_HOURS,
      session_id = cumsum(new_session)
    ) |>
    ungroup() |>
    mutate(session_key = paste(customer_id, session_id, sep = "_"))
}

# Apply session creation
google_analytics_sessions <- create_sessions(google_analytics)

cat("Session timeout set to:", SESSION_TIMEOUT_HOURS, "hours\n")
cat("Total sessions created:", n_distinct(google_analytics_sessions$session_key), "\n\n")

# SESSION-LEVEL CONVERSION STEPS

# Classify each session by what happened
session_summary <- google_analytics_sessions |>
  group_by(session_key, customer_id, session_id) |>
  summarise(
    session_start = min(event_timestamp),
    session_end = max(event_timestamp),
    session_duration_mins = as.numeric(difftime(max(event_timestamp), min(event_timestamp), units = "mins")),
    num_events = n(),
    
    # Key actions in session
    had_add_to_cart = any(event_name == "add_to_cart"),
    had_view_cart = any(event_name == "view_cart"),
    had_proceed_to_checkout = any(event_name == "proceed_to_checkout"),
    had_begin_checkout = any(event_name == "begin_checkout"),
    had_add_payment = any(event_name == "add_payment_info"),
    had_purchase = any(event_name == "purchase"),
    had_remove_from_cart = any(event_name == "remove_from_cart"),
    
    # Counts
    add_to_cart_count = sum(event_name == "add_to_cart"),
    remove_from_cart_count = sum(event_name == "remove_from_cart"),
    update_cart_count = sum(event_name == "update_cart"),
    
    # Device and errors
    device = first(device_category),
    had_error = any(grepl("Failed|Error|failed", event_name)),
    
    .groups = "drop"
  ) |>
  mutate(
    # Session outcome classification
    session_type = case_when(
      had_purchase ~ "Converted",
      had_begin_checkout ~ "Checkout Abandoned",
      had_proceed_to_checkout ~ "Cart Abandoned (Late)",
      had_view_cart ~ "Cart Abandoned (Early)",
      had_add_to_cart ~ "Added but Never Viewed Cart",
      TRUE ~ "Browse Only"
    ),
    session_type = factor(session_type, levels = c(
      "Browse Only", "Added but Never Viewed Cart", 
      "Cart Abandoned (Early)", "Cart Abandoned (Late)", 
      "Checkout Abandoned", "Converted"
    ))
  )

# Session outcome distribution
session_outcomes <- session_summary |>
  count(session_type) |>
  mutate(pct = n / sum(n) * 100)

ggplot(session_outcomes, aes(x = session_type, y = n, fill = session_type)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = paste0(comma(n), "\n(", round(pct, 1), "%)")), 
            vjust = -0.5, size = 3) +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.4))) +
  scale_fill_manual(values = c("grey70", "orange", "coral", "red", "darkred", "darkgreen")) +
  labs(title = "Session Outcomes: What Happens in Each Window?",
       subtitle = paste0("Total sessions: ", comma(nrow(session_summary)), 
                        " | Session timeout: ", SESSION_TIMEOUT_HOURS, " hours"),
       x = NULL, y = "Number of Sessions") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1))

```

- This showed that we had 78,813 total "sessions" in the dataset. 
- ~ 50% of these were sessions that never involved a customer adding an item to their cart
- ~ 40% of these ended in full conversion with a purchase by the customer
- It seems that many of the in-between events were not necessarily required to reach the full purchase.

## Device Analysis
- Which devices were customers using to access the platform? 
- Are there any issues with the current distribution of platform usage across devices? 
```{r}

# Device category breakdown
device_summary <- google_analytics |>
  count(device_category, sort = TRUE) |>
  mutate(pct = n / sum(n) * 100)

ggplot(device_summary, aes(x = reorder(device_category, n), y = n, fill = device_category)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = paste0(comma(n), "\n(", round(pct, 1), "%)")), 
            hjust = -0.1, size = 3.5) +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Events by Device Category",
       subtitle = "Desktop vs Mobile vs Tablet usage",
       x = NULL, y = "Number of Events") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

# Operating system distribution
os_summary <- google_analytics |>
  count(device_operating_system, sort = TRUE) |>
  head(10)

ggplot(os_summary, aes(x = reorder(device_operating_system, n), y = n)) +
  geom_col(fill = "purple") +
  geom_text(aes(label = comma(n)), hjust = -0.1, size = 3) +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
  labs(title = "Top 10 Operating Systems",
       x = NULL, y = "Number of Events") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

# Mobile brand analysis (for mobile devices only)
mobile_brands <- google_analytics |>
  filter(device_category == "mobile") |>
  count(device_mobile_brand_name, sort = TRUE) |>
  head(10)

ggplot(mobile_brands, aes(x = reorder(device_mobile_brand_name, n), y = n)) +
  geom_col(fill = "orange") +
  geom_text(aes(label = comma(n)), hjust = -0.1, size = 3) +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
  labs(title = "Top 10 Mobile Device Brands",
       subtitle = "Among mobile device users",
       x = NULL, y = "Number of Events") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))


```

- The majority of users access the mycoke 360 platform from their desktop but we do have a decent subset of users who use the platform on mobile, **16.6%**. 
- Most users access the site using Windows with Apple's IOS and Mac OS following in 2nd and 3rd most popular.
- Among mobile devices Apple is the most common with Samsung and Google next in line.

## Customer Behavior
- How often do customers use the platform?
```{r}

# Events per customer distribution
events_per_customer <- google_analytics |>
  count(customer_id, name = "num_events") |>
  mutate(engagement_level = case_when(
    num_events <= 50 ~ "Light user",
    num_events <= 500 ~ "Moderate user",
    num_events <= 1000 ~ "Active user",
    TRUE ~ "Power user"
  ))

# Events per customer log scale histogram 
ggplot(events_per_customer, aes(x = num_events)) +
  geom_histogram(bins = 50, fill = "steelblue", color = "white") +
  scale_x_log10(labels = comma) +
  scale_y_continuous(labels = comma) +
  labs(title = "Distribution of Events per Customer",
       subtitle = "How engaged are different customers? (log scale)",
       x = "Number of Events (log scale)", y = "Number of Customers") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

# Engagement level summary
engagement_summary <- events_per_customer |>
  count(engagement_level) |>
  mutate(pct = n / sum(n) * 100,
         engagement_level = factor(engagement_level, 
                                   levels = c("One-time visitor", "Light user", 
                                             "Moderate user", "Active user", "Power user")))

# Enngagement level plot
ggplot(engagement_summary, aes(x = engagement_level, y = n, fill = engagement_level)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = paste0(comma(n), "\n(", round(pct, 1), "%)")), 
            vjust = -0.3, size = 3) +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.1))) +
  scale_fill_brewer(palette = "YlOrRd") +
  labs(title = "Customer Engagement Levels",
       subtitle = "Distribution of customers by activity level",
       x = NULL, y = "Number of Customers") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1))

```

- The majority of customers have around 1000 google analytics events throughout the year 
- After Binning the users we can see that most customers fall into the 500-1000 events per year group.

## Page Analysis
- Which pages were the most frequently visited on the site? 
```{r}

# Most visited pages (excluding null)
page_summary <- google_analytics |>
  filter(event_page_name != "null", !is.na(event_page_name)) |>
  count(event_page_name, sort = TRUE) |>
  head(15)

ggplot(page_summary, aes(x = reorder(event_page_name, n), y = n)) +
  geom_col(fill = "darkblue") +
  geom_text(aes(label = comma(n)), hjust = -0.1, size = 3) +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
  labs(title = "Top 15 Most Visited Pages",
       subtitle = "Which pages are getting the most traffic?",
       x = NULL, y = "Number of Page Views") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

```

- The MyCoke Orders Page had the most traffic by far.
- This shows that many customers think about and modify their orders many times before actually reaching a purchase event. 
- Purchase events accounted for **94,115** total events.


# üì¶ Orders Table EDA

```{r}
# quick view
glimpse(orders)

```

## Order Volume
- Which months had the most orders? 
```{r}

# Monthly order volume
monthly_orders <- orders |>
  filter(created_date_est > as.Date("2024-05-31")) |> # May has hardly anything and doesn't add to the viz
  mutate(month = floor_date(created_date_est, "month")) |>
  count(month, name = "num_orders")

ggplot(monthly_orders, aes(x = month, y = num_orders)) +
  geom_col(fill = "steelblue") +
  geom_text(aes(label = comma(num_orders)), vjust = -0.5, size = 2.5) +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.1))) +
  labs(title = "Monthly Order Volume",
       subtitle = "Total orders per month",
       x = "Month", y = "Number of Orders") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1))

```

- This looks pretty even throughout the year with a little dip in November and December

## Order Type Analysis
- How are customers placing orders? 
- Are we seeing customers use the mycoke360 platform more often? 
```{r}
# Order type distribution
order_type_summary <- orders |>
  count(order_type, sort = TRUE) |>
  mutate(pct = n / sum(n) * 100)

ggplot(order_type_summary, aes(x = reorder(order_type, n), y = n, fill = order_type)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = paste0(comma(n), "\n(", round(pct, 1), "%)")), 
            hjust = -0.1, size = 3.5) +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
  scale_fill_brewer(palette = "Set2") +
  labs(title = "Orders by Order Type",
       subtitle = "How are orders being placed?",
       x = NULL, y = "Number of Orders") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

# Order type trend over time
order_type_trend <- orders |>
  mutate(month = floor_date(created_date_est, "month")) |>
  count(month, order_type) |>
  group_by(month) |>
  mutate(pct = n / sum(n) * 100) |>
  ungroup()

ggplot(order_type_trend, aes(x = month, y = pct, color = order_type, group = order_type)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  scale_y_continuous(labels = comma) +
  scale_color_brewer(palette = "Set2") +  
  labs(title = "Order Type Trends Over Time",
       subtitle = "Is digital adoption increasing?",
       x = "Month", y = "% of Orders", color = "Order Type") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1))
```

- These show that while Sales Reps are still taking most of the order share, the **MyCoke360 platform has increased in usage** as the year has gone on.

## Order Quantity Table 
```{r}
# Create the table 
quantity_summary <- orders |>
  summarise(
    `Mean Order Quantity` = round(mean(order_quantity, na.rm = TRUE), 2),
    `Median Order Quantity` = median(order_quantity, na.rm = TRUE),
    `Max Order Quantity` = max(order_quantity, na.rm = TRUE),
    `Total Units Ordered` = sum(order_quantity, na.rm = TRUE)
  ) |>
  pivot_longer(everything(), names_to = "Metric", values_to = "Value") |>
  mutate(Value = ifelse(Metric %in% c("Max Order Quantity", "Total Units Ordered"),
                        comma(Value), as.character(Value)))

# Desplay in a nice Kable HTML table
kbl(
  quantity_summary,
  format = "html",
  caption = "Order Quantity Summary",
  align = c("l", "r")
) |>
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed", "responsive"),
    full_width = FALSE,
    position = "center"
  )

```


## Product (Material) Analysis
- Which products were most popular? 
```{r}

# Join the materials table so we can learn more about specific product trends beyond ID
orders_material <- orders |>
  left_join(material, by = "material_id") |> 
    filter(!is.na(bev_cat_desc), # Filter out the NA and "" categories 
          bev_cat_desc != "")

# ---- Category-level summary ----
category_summary <- orders_material |>
  group_by(bev_cat_desc) |>
  summarise(
    total_orders = n(),
    total_quantity = sum(order_quantity, na.rm = TRUE),
    unique_products = n_distinct(material_id),
    unique_customers = n_distinct(customer_id),
    .groups = "drop"
  ) |>
  arrange(desc(total_orders))


# ---- Plot: Category-level volume ----
ggplot(category_summary, aes(x = reorder(bev_cat_desc, total_orders), y = total_orders, fill = bev_cat_desc)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = comma(total_orders)), hjust = -0.1, size = 3) +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
  labs(
    title = "Order Volume by Beverage Category",
    subtitle = "Aggregated across all materials",
    x = "Beverage Category",
    y = "Number of Orders"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))


# Product diversity
product_diversity <- orders_material |>
  group_by(customer_id) |>
  summarise(unique_products = n_distinct(material_id), .groups = "drop")

# Filter to less than 100
ggplot(product_diversity, aes(x = unique_products)) +
  geom_histogram(bins = 30, fill = "orange", color = "white") +
  coord_cartesian(xlim = c(0, 100)) +  # I set this to 100 because most customers have less than that 
  scale_y_continuous(labels = comma) +
  labs(
    title = "Product Diversity per Customer",
    subtitle = "Most customers order fewer than 100 unique products",
    x = "Number of Unique Materials Ordered",
    y = "Number of Customers"
  ) +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))


```

- The most popular category is "Core Sparkling" this includes sodas which make up the majority of orders.
-  Most customers purchase less than 100 different items across all orders, this tells us that the majority of the customers do not drastically modify the items they order throughout the year. 

## Top Products by Category Table 

- Click the tab below to see the full table.

<button class="accordion-button" type="button" data-bs-toggle="collapse" data-bs-target="#collapseOne" >Click to See Top Products Table</button><div id="collapseOne" class="accordion-collapse collapse"><div>

```{r}
# Top products table

# Top 3 products per category
top_products_by_cat <- orders_material |>
  group_by(bev_cat_desc, material_id, trade_mark_desc, flavour_desc, pack_size_desc) |>
  summarise(
    num_orders = n(),
    total_quantity = sum(order_quantity, na.rm = TRUE),
    .groups = "drop"
  ) |>
  arrange(bev_cat_desc, desc(num_orders)) |>
  group_by(bev_cat_desc) |>  # per category
  slice_head(n = 3) |>
  ungroup() |>                # remove grouping before table
  mutate(Product = paste0(trade_mark_desc, " - ", flavour_desc, " (", pack_size_desc, ")")) |>
  select(Beverage_Category = bev_cat_desc, Product, `# Orders` = num_orders, `Total Quantity` = total_quantity)

# Create HTML table
top_products_by_cat |>
  kable(format = "html", escape = FALSE, align = "lccc") |>
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed")) |>
  add_header_above(c(" " = 1, "Top Products per Beverage Category" = 3))


```

</div></div>


## Plant/Fulfillment Analysis
- Which plants process the most orders? 
```{r}
# Orders by plant
plant_summary <- orders |>
  count(plant_id, sort = TRUE) |>
  head(20) |>
  mutate(pct = n / sum(n) * 100)

ggplot(plant_summary, aes(x = reorder(plant_id, n), y = n)) +
  geom_col(fill = "darkblue") +
  geom_text(aes(label = comma(n)), hjust = -0.1, size = 3) +
  coord_flip() +
  scale_y_continuous(labels = comma, expand = expansion(mult = c(0, 0.15))) +
  labs(title = "Top 20 Plants by Order Volume",
       subtitle = "Which fulfillment centers handle the most orders?",
       x = "Plant ID", y = "Number of Orders") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14))

# Plant workload over time
top_plants <- orders |> count(plant_id, sort = TRUE) |> head(10) |> pull(plant_id)

plant_trend <- orders |>
  filter(plant_id %in% top_plants) |>
  mutate(month = floor_date(created_date_est, "month")) |>
  count(month, plant_id)

ggplot(plant_trend, aes(x = month, y = n, color = plant_id, group = plant_id)) +
  geom_line(size = 0.8) +
  scale_y_continuous(labels = comma) +
  labs(title = "Order Volume Trends for Top 10 Plants",
       subtitle = "How has workload distribution changed?",
       x = "Month", y = "Number of Orders", color = "Plant ID") +
  theme_minimal() +
  theme(plot.title = element_text(face = "bold", size = 14),
        axis.text.x = element_text(angle = 45, hjust = 1))

```

- Plant G111 stays very busy throughout the whole year processing most of the orders for the company. (192,000 orders)
- There are a few other plants that process around 100,000 per year with many others below 100,000.

***

# üí∞ Sales Table EDA

## Revenue and Profit Trends

```{r}

# Monthly sales
monthly_sales <- sales |>
  mutate(posting_date = mdy(posting_date),   # convert to Date
         month = floor_date(posting_date, "month")) |>
  filter(posting_date > as.Date("2024-05-31")) |> # May has hardly anything and doesn't add to the viz

  group_by(month) |>
  summarise(
    revenue = sum(nsi_dead_net, na.rm = TRUE),
    profit = sum(gross_profit_dead_net, na.rm = TRUE),
    profit_margin = profit / revenue * 100,
    .groups = "drop"
  )

ggplot(monthly_sales, aes(x = month)) +
  geom_col(aes(y = revenue), fill = "steelblue", alpha = 0.7) +
  geom_line(aes(y = profit), color = "darkgreen", size = 1.5) +
  scale_y_continuous(labels = dollar) +
  labs(title = "Monthly Revenue (bars) & Profit (line)", x = "Month", y = NULL) +
  theme_minimal()

```

- Revenue and profits don't drastically change across months.

## Customer Value Analysis
- Which segments of customers contribute most to overall revenue and profit, and how concentrated is revenue among top-tier customers?

```{r}
# Customer-level metrics
# this lets us see where the money is coming from for orders 
customer_value <- sales |>
  group_by(customer_id) |>
  summarise(
    total_revenue = sum(nsi_dead_net, na.rm = TRUE),
    total_profit = sum(gross_profit_dead_net, na.rm = TRUE),
    num_transactions = n(),
    unique_products = n_distinct(material_id),
    avg_transaction_value = mean(nsi_dead_net, na.rm = TRUE),
    .groups = "drop"
  ) |>

  # Now lets group the customers into tiers based on their revenue
  mutate(
    customer_tier = case_when(
      total_revenue >= quantile(total_revenue, 0.9) ~ "Top 10%",
      total_revenue >= quantile(total_revenue, 0.75) ~ "Top 25%",
      total_revenue >= quantile(total_revenue, 0.5) ~ "Top 50%",
      TRUE ~ "Bottom 50%"
    )
  )

# Customer tier distribution
tier_summary <- customer_value |>
  group_by(customer_tier) |>
  summarise(
    customers = n(),
    revenue = sum(total_revenue),
    profit = sum(total_profit),
    .groups = "drop"
  ) |>
  mutate(
    pct_customers = customers / sum(customers) * 100,
    pct_revenue = revenue / sum(revenue) * 100
  )

ggplot(tier_summary, aes(x = reorder(customer_tier, -revenue), y = revenue, fill = customer_tier)) +
  geom_col(show.legend = FALSE) +
  geom_text(aes(label = paste0(dollar(revenue, scale = 1e-6, suffix = "M"), "\n", 
                               round(pct_revenue, 1), "%")), vjust = -0.5, size = 3.5) +
  scale_y_continuous(labels = dollar, expand = expansion(mult = c(0, 0.4))) +
  labs(title = "Revenue by Customer Tier", x = "Customer Tier", y = "Total Revenue") +
  theme_minimal()

```

- Almost **80%** of all revenue comes from the top 10% of customers.
- If we can focus on cart abandonment for these customers it will have the most impact on sales. 
- The bottom 50% of customers produces only 2.8% of revenue. 

## Product Revenue
- Which products produce the most revenue?
```{r}

# Merge product info
sales_material <- sales |>
  left_join(material, by = "material_id") |> 
  filter(!is.na(bev_cat_desc), # Filter out the NA and "" categories 
          bev_cat_desc != "")

# Category-level metrics 
category_performance <- sales_material |>
  group_by(bev_cat_desc) |>
  summarise(
    revenue = sum(nsi_dead_net, na.rm = TRUE),
    profit = sum(gross_profit_dead_net, na.rm = TRUE),
    volume = sum(physical_volume, na.rm = TRUE),
    transactions = n(),
    customers = n_distinct(customer_id),
    .groups = "drop"
  ) |>
  mutate(profit_margin = profit / revenue * 100)

# Plot by Category
ggplot(category_performance, aes(x = reorder(bev_cat_desc, revenue), y = revenue, fill = bev_cat_desc)) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_y_continuous(labels = scales::dollar) +
  labs(
    title = "Revenue by Beverage Category",
    subtitle = "Aggregated by category across all materials",
    x = "Beverage Category",
    y = "Revenue"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(face = "bold", size = 14),
    axis.text.y = element_text(size = 10)
  )

```

- Similar to the order quantities, the "Core Sparkling" group produced the most revenue, outpacing other categories by over $20 million.



# üìä Results 

## Overview 
The datasets we have been given for this project provide a detailed look at how Swire creates revenue by providing customers with many options to place orders and receive products. We wish that we had more time to finish creating the target variable, but the complexity of the data wrangling task didn't allow us to explore this yet. We look forward to exploring how cart abandonment relates to the analysis we have done in this notebook. 

## Data Quality Issues 
- While some of the data was usable and didn't require much cleaning there were some tables that raised concerns:
    - There was some inconsistency with frequency labeling across tables.
    - Inconsistency with distribution modes across tables with some only existing in 1 table and not existing in others.
    - Some visit plan anchor dates had overlapping snapshot dates with other anchor dates for the same customer.
    - Time zones were inconsistent across tables and had to be matched to local time for analysis. 
    - There were some null and "" values in the categories for some of the products in the materials table.

## Key Patterns and Insights
- As we explored the data, we observed that the MyCoke360 platform has started to become more popular as more customers transition from other methods like sales reps and call centers. 
- While a small subset of customers uses mobile (14%), the majority (85%) of users interact with the website using their desktop. This signifies the importance of optimizing the desktop experience to avoid cart abandonment there. 
- The "core sparkling" product category accounts for most of the order volume and revenue for the business.
- Order volume and revenue is constant by month throughout the year with small variation. 
- Almost 80% of total revenue is produced by 10% of the customers. If we can focus on the cart abandonment trends in this group, we will be able to make the most impact on company profits. 
